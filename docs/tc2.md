% CS 452 Train Control Milestone 2
% Benjamin Zhao, Kyle Verhoog
% $date$


# Operation

The elf file is located at `/u/cs452/tftp/ARM/ktverhoo/tc2.elf` and can
be initiated using the typical `load` command.

After `load`, wait a few seconds for tasks to initialize.

You should be presented with our interface featuring _windows_.

You can type commands described in the next section.

## Commands

- `tr <tr#> <speed>` sets the given train to the given speed.
- `dr <tr#> <>` sets the given train to the given speed.
- `dummy <offx> <offy> <width> <height>` creates a dummy task which writes to
  the screen in a new window.
- `cal <train> <speed> <pivot>` stopping calibration command, uses pivot as the
- `ms <train>` calibrates the inching speed using train speed 4.
- `tst <train> <node>` tests the calibrated test on a node of your choice.


# Design

## The Pipeline

We spent a long time and had a number of extended conversations with each other
regarding how to design a clean, scalable, testable solution to the problem of
interpreting track events. Our solution was to come up with a data pipeline
which provides layers of abstraction to real and virtual events.

### Events

#### Raw

We name events that emanate from the real world "raw events". These include `tr`
commmands, `sw` commands and results from sensor polls.


#### Virtual

We name events that are generated by our system (which are fed back into itself)
virtual events. These events may correspond to raw events or may not. They help
maintain program state that is asynchronous or requires timing.

Virtual events allow our program to achieve a finer grained granularity than
mere sensor poll updates. This allows subscriber tasks to subscribe on events on
track nodes that are not sensors.

#### Types
The waiting room will return different event types depending on the situation of
the anticipated event. Virtual Events can pre-emptively register to the waiting
room that the virtual event is coming up known as a VRE. When a raw event
corresponding to the virtual event, it raises an RE. When the virtual event
comes in, a VE is raised. Combining the three, we obtain the combinations (VRE
VE), (VRE RE), (VRE VE RE).

`VRE VE` - Virtual Event came in, timed-out without a corresponding Raw Event
`VRE RE` - Virtual Event Registered, Raw Event came before estimated Virtual Event
`VRE VE RE` - Virtual Event came, Raw Event came shortly after (within an alloted threshold)

### Providers

#### Data Providers

The data providers are tasks which primarily focus on talking directly with the
train controller to issue commands or poll on sensor data. These tasks form into
three major groups.

#### TrainProvider

The train provider entity is a set of tasks which performs the operations of
sending atomic train commands to the train controller. The train controller
listens on any incoming requests made by other tasks and sends the request to
the train controller. In addition, the TrainProvider raises a raw event to the
pipeline signalling that an event to move a train has been issued. Subscribers
on the raw event are notified when the event takes place. Currently, only the
WaitingRoom task subscribes to the TrainProvider.

#### SwitchProvider

Similar to the TrainProvider, the SwitchProvider is a set of tasks performing
sends and raising of events for when switch requests occur, The SwitchProvider
listens on any incoming requests and executes them. It raises an event and
notifies all subscribers that a raw event for a change on a specific switch has
occured. Currently, only the WaitingRoom task subscribes to the SwitchProvider

#### SensorProvider

The SensorProvider is a set of tasks which manage the polling of sensor data.
All round trips of sensor data are raised as a raw event for the subscribers to
listen for. On a further note the WaitingRoom which subscribes to the
SensorProvider sits behind a SensorDelta, which filters the raw events for only
the iterations which differ from its previous.

#### VirtualEventProvider

Along side the raw data provider, TheVrtualEvent provider raises virtual events
to subscribers. Virtual event registrations are periodically delivered to
subscribers. Note that VirtualEvents are not affiliated with TrainController
itself, but can provide accurate measurements and predictions for when Raw
Events may actually occur.


### Waiting Room / Matchmaker

The waiting room is a precursor to the interpreter. Many Virtual Events have
direct correlations with a Raw Event. These virtual events will wait in the
waiting room for the corresponding raw events to occur. At the worst case, raw
events which do not occur (ie broken sensor) can be pseudo reflected by the
virtual event and will be up to the interpreter to decide an acceptable state.

### Interpreter

The Interpreter is where most of the logic is performed for figuring out, given
both virtual and raw events, what is actually going on, on the track.

We took what we consider to be an interesting approach of associating trains to
sensors. We make no assumptions about where and when the train will be. The
interpreter assumes that anything can happen and then tries to make sense of the
data it gets back.

If irregularities are detected (for example, a train appears to go down both
paths of a branch) then the Interpreter invalidates the reading it got and
places the train in a `TR_LOST` state. When new events emerge, we check
`TR_LOST` trains attempting to re-associate the train.

The Interpreter generates higher level events to pass on to the Representor
which then goes on to distribute them to subscribers. Events the Interpreter
generates include train $i$ at position $n$.

# Acceleration, Velocity and Distance

In order to modularize at a higher precision based on train velocity, a straight
mapping between train setting and train velocity is not enough. An easy way to
interpolate the velocity between a speed setting is to apply a
`(n-1)`-degree polynomial interpolation (in our case we used the lagrange
method). Since integrating and deriving polynomials are systematic, we can
easily attribute acceleration to velocity to distance to a confident degree of
accuracy.

Note that interpolation may fluctuate at higher valued results, and thus it may
be worth while to apply partial interpolation instead.




### Representor

The Representor is a high level event provider that gives subscribing tasks an
API to by notified of current events. It essentially passes on some events from
the Interpreter with additional computed events.


### Subscribers

The subscribers form the bottom of the pipeline. They subscribe to events
provided by the Representor. These events are high level events like train `24`
at node `B13`.

Some examples of subscribers are the interfaces for the track events. The
sensor, switch and train interfaces each subscribe to events provided by the
Representor.

### Reservation Manager

The Reservation Manager provides a mechanism for trains to allocate track nodes
to attempt to avoid collisions.

Our Reservation Manager is extremely conservative, we reserve the stopping
distance of the train and a little more at the granularity of sensors for all
possible paths ahead of us. That is, a reservation is made between two sensors.
So even if the stopping distance of the train is 1mm, we will reserve all the
way to the next sensor in each direction possible.

This is due to the fact that we did not have time to get a finer grained update
granularity functional for this milestone.

Trains request track nodes ahead of it and return nodes that it has passed by.
Due to the nature of the interpreter model and since the train can potentially
be reset a node backwards we free two nodes behind us.

An API for pathing was attempted but not achieved. This API considers current
reservations and attempts to path around them.

#### Train Driver

The Train Driver does just that, drives the train. It is in charge of attempting
to allocate and free nodes using the Reservation Manager. If it cannot allocate
nodes from the Reservation Manager then it sends the stop command to the train
and attempts to reverse and path in the opposite direction.

The Train Driver also keeps track of the state of the train it is driving by
subscribing to the Representor. It attempts to path its train to a given
position. Once the train has arrived at the position, it re-routes the train to
the reversed position. For example, if we route the train to node `A15`, once
the train has attempted to stop at `A15` it will re-route to `A16`.

## Display Manager

We figured that it would be worth it to have a structured way to present data to
the screen both for presentation as well as for debugging.

### Terminal Manager

Terminal Manager manages a set of windows. Tasks can request a window from the
Terminal Manager and output from the task can be redirected to either its own
window or a common-to-all-tasks logging window.

There is currently support (not enabled currently) to route input to windows
other than the shell, depending on the cursor focus.

The Terminal Manager attempts to smartly render the screen as to limit the
amount of cursor movement byte-sequences needed.

### Shell

Shell is just the first task which registers to the terminal manager which is
configured to accept input. It is currently a monstrosity which handles the
parsing and executing of all commands.

## IOServer

### Blocking PutC

We added a blocking version of the `PutC` function which is very useful in
applications like the Sensor Manager for when it polls to the train controller.

## TC 2

### Res

After calibrating, we manually store the stopping distances. The stopping logic
is relatively straightforward. We step backwards from the last node until we
find the first sensor that is more than the stopping distance away from the
node. Then after passing this sensor we can set a delay to stop at the correct
position.

### Calibrating

#### Method 1

Our intial method to calibrate train stopping distances was to have the train
use a starting sensor as a measuring stick to try to land on a targer sensor
further on down the track. We make an intial guess and check whether or not we
overshoot the target. We then subtract or add to our guess and bring the train
around to try again.

Obviously this is not very efficient and took quite a while to run, taking up to
5 or 6 iterations to achieve accurate results.

The plus side to this technique was that the results we get from the test are
quite accurate.

#### Method 2

Our more efficient method was an inching strategy. Again, we start out with a
guess. But this time if we overshoot the sensor we inch at a slow speed that we know
to the next sensor, measuring how much time it takes. By using a slower speed we
make the assumption of very little acceleration and thus we can calculate the
stopping distance using the time and the speed.

This method is much more efficient and only takes a couple of iterations to get
good results.

A problem, not with the method, is that moving at a slow speed means a greater
chance of getting stuck, which ruins calibration results.

### Pathing

Pathing is done using Dijstra's algorithm using our own heap implementation.
There is nothing particularly special about the algorithm or its implementation.

It is thoroughly unit tested and we are fairly confident in its correctness.

When given a node to stop at, we calculate the path starting two sensors ahead of
the train to the destination node. Then we check the switches in the path and
set them in order starting from the destination node.

### Resetting

We have a simple reset task which configures the switches to form a loop which
we use to set ourselves up for the pathing and stopping.

# Files/Hashes

The code is in the `tc2` branch at
`https://git.uwaterloo.ca/bkcs452/kernel/commits/tc2`

